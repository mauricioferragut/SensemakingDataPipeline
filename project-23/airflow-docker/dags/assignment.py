from airflow import DAG
from datetime import timedelta
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
import urllib.request
import time
import glob, os
import json

#Task 1
#catalog function pulls data from url, creates and writes to file
def catalog():
    #pull helper function, pulls data from url
    def pull(url):
        response = urllib.request.urlopen(url).read()
        data = response.decode('utf-8')
        return data

    #data is returned by pull() function, file is filename
    def store(data, file):
        #create and open file named after url where data came from (Ex: m1a.html)
        with open(file, 'w') as writer:
            #write the data to the file
            writer.write(data)
        
        print(f'Wrote file: {file}')

    #create url list from provided txt file
    urls = ['http://student.mit.edu/catalog/m1a.html',
    'http://student.mit.edu/catalog/m1b.html',
    'http://student.mit.edu/catalog/m1c.html',
    'http://student.mit.edu/catalog/m2a.html',
    'http://student.mit.edu/catalog/m2b.html',
    'http://student.mit.edu/catalog/m2c.html',
    'http://student.mit.edu/catalog/m3a.html',
    'http://student.mit.edu/catalog/m3b.html',
    'http://student.mit.edu/catalog/m4a.html',
    'http://student.mit.edu/catalog/m4b.html',
    'http://student.mit.edu/catalog/m4c.html',
    'http://student.mit.edu/catalog/m4d.html',
    'http://student.mit.edu/catalog/m4e.html',
    'http://student.mit.edu/catalog/m4f.html',
    'http://student.mit.edu/catalog/m4g.html',
    'http://student.mit.edu/catalog/m5a.html',
    'http://student.mit.edu/catalog/m5b.html',
    'http://student.mit.edu/catalog/m6a.html',
    'http://student.mit.edu/catalog/m6b.html',
    'http://student.mit.edu/catalog/m6c.html',
    'http://student.mit.edu/catalog/m7a.html',
    'http://student.mit.edu/catalog/m8a.html',
    'http://student.mit.edu/catalog/m8b.html',
    'http://student.mit.edu/catalog/m9a.html',
    'http://student.mit.edu/catalog/m9b.html',
    'http://student.mit.edu/catalog/m10a.html',
    'http://student.mit.edu/catalog/m10b.html',
    'http://student.mit.edu/catalog/m11a.html',
    'http://student.mit.edu/catalog/m11b.html',
    'http://student.mit.edu/catalog/m11c.html',
    'http://student.mit.edu/catalog/m12a.html',
    'http://student.mit.edu/catalog/m12b.html',
    'http://student.mit.edu/catalog/m12c.html',
    'http://student.mit.edu/catalog/m14a.html',
    'http://student.mit.edu/catalog/m14b.html',
    'http://student.mit.edu/catalog/m15a.html',
    'http://student.mit.edu/catalog/m15b.html',
    'http://student.mit.edu/catalog/m15c.html',
    'http://student.mit.edu/catalog/m16a.html',
    'http://student.mit.edu/catalog/m16b.html',
    'http://student.mit.edu/catalog/m18a.html',
    'http://student.mit.edu/catalog/m18b.html',
    'http://student.mit.edu/catalog/m20a.html',
    'http://student.mit.edu/catalog/m22a.html',
    'http://student.mit.edu/catalog/m22b.html',
    'http://student.mit.edu/catalog/m22c.html']
    
    #iterate through url list, pull from each and write file for each
    for url in urls:
        index = url.rfind('/') + 1
        file = url[index:]
        store(pull(url), file)
        print(f'Pulled: {file}')
        print('--- waiting ---')
        time.sleep(5)
    

#Task 2
#combine all unstructured data files into one large file
def combine():
    with open('combo.txt', 'w') as outfile:
       #iterate through all files ending in .html
       for file in glob.glob("*.html"):
           with open(file) as infile:
               outfile.write(infile.read())


#Task 3
#Parse out titles from <h3> tags
def titles():
    from bs4 import BeautifulSoup
    #helper function
    def store_json(data,file):
       with open(file, 'w', encoding='utf-8') as f:
           json.dump(data, f, ensure_ascii=False, indent=4)
           print(f'Wrote file: {file}')

    #Open and read the large html file generated by combine()
    with open('combo.txt', 'r') as html:
        html = html.read()
        #the following replaces new line and carriage return char
        html = html.replace('\n', ' ').replace('\r', '')
        #the following creates an html parser
        soup = BeautifulSoup(html, "html.parser")
        results = soup.find_all('h3')
        titles = []

        # tag inner text
        for item in results:
            titles.append(item.text)
        store_json(titles, 'titles.json')

#Task 4
#remove all punctuation, numbers, and one-character words from the titles.json file
def clean():
    def store_json(data,file):
       with open(file, 'w', encoding='utf-8') as f:
           json.dump(data, f, ensure_ascii=False, indent=4)
           print(f'Wrote file: {file}')

    with open('titles.json') as file:
       titles = json.load(file)
       # remove punctuation/numbers
       for index, title in enumerate(titles):
           punctuation= '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
           translationTable= str.maketrans("","",punctuation)
           clean = title.translate(translationTable)
           titles[index] = clean

       # remove one character words
       for index, title in enumerate(titles):
           clean = ' '.join( [word for word in title.split() if len(word)>1] )
           titles[index] = clean

        # remove [and, of, to, in, the, ]
       word_list = ['and', 'of', 'to', 'in', 'the', 'The', 'for']
       for index, title in enumerate(titles):
           clean = ' '.join( [word for word in title.split() if word not in word_list] )
           titles[index] = clean

       store_json(titles, 'titles_clean.json')


#Task 5
#Count word frequency
def count_words():
     from collections import Counter
     def store_json(data,file):
       with open(file, 'w', encoding='utf-8') as f:
           json.dump(data, f, ensure_ascii=False, indent=4)
           print(f'Wrote file: {file}')

     with open('titles_clean.json') as file:
            titles = json.load(file)
            words = []

            # extract words and flatten
            for title in titles:
                words.extend(title.split())

            # count word frequency
            counts = Counter(words)
            store_json(counts, 'words.json')


#Design Airflow pipeline
with DAG(
   "assignment",
   start_date=days_ago(1),
   schedule_interval="@daily",catchup=False,
) as dag:

# INSTALL BS4 BY HAND THEN CALL FUNCTION

   # ts are tasks
   t0 = BashOperator(
       task_id='task_zero',
       bash_command='pip install beautifulsoup4',
       retries=2
   )
   t1 = PythonOperator(
       task_id='task_one',
       depends_on_past=False,
       python_callable=catalog
   )
   t2 = PythonOperator(
       task_id='task_two',
       depends_on_past=False,
       python_callable=combine
   )
   t3 = PythonOperator(
       task_id='task_three',
       depends_on_past=False,
       python_callable=titles
   )
   t4 = PythonOperator(
       task_id='task_four',
       depends_on_past=False,
       python_callable=clean
   )
   t5 = PythonOperator(
       task_id='task_five',
       depends_on_past=False,
       python_callable=count_words
   )

   t0>>t1>>t2>>t3>>t4>>t5